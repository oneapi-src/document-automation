{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ba9d3f",
   "metadata": {},
   "source": [
    "## **Reference Implementation**\n",
    "### ***E2E Architecture***\n",
    "### **Use Case E2E flow**\n",
    "\n",
    "<img src=\"assets/e2e_flow.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5e984a",
   "metadata": {},
   "source": [
    "### ***Solution setup***\n",
    "Check that you are in the `ner_stock` kernel. If not, navigate to `Kernel > Change kernel > Python [conda env:ner_stock]` \\\n",
    "After activating the environment for stock Tensorflow framework, make sure the oneDNN flag is disabled by running the below cell to set the flag and verify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export TF_ENABLE_ONEDNN_OPTS=0\n",
    "echo $TF_ENABLE_ONEDNN_OPTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a923058",
   "metadata": {},
   "source": [
    "### ***Solution implementation***\n",
    "#### **Model building process**\n",
    "The cell below will execute a python script to initiate training in the kernel enabled above\n",
    "\n",
    "The script will run the benchmarks for the passed parameters and displays the corresponding  training time in seconds. The details of the script and parameters are as follows:\n",
    "\n",
    "```shell\n",
    "python src/run_modeltraining.py --batchsize <batchsize value> --dataset_file <dataset filename> -i <intel/stock> --save_model_path <save file path>\n",
    "\n",
    "```\n",
    "\n",
    "  Arguments:<br>\n",
    "```\n",
    "  --help                   show this help message and exit\n",
    "  --batchsize              Give the required batch sizes\n",
    "  --dataset_file           Give the name of dataset file\n",
    "  --intel                  0 for stock, 1 for intel environment\n",
    "  --save_model_path        Give the directory path to save the model after the training\n",
    "```\n",
    "\n",
    "Optionally, users can collect logs by redirecting to an output file as follows:\n",
    "```shell\n",
    "python src/run_modeltraining.py --batchsize <batchsize value> --dataset_file <dataset filename> -intel <intel/stock> --save_model_path <save file path> | tee <log_file_name>\n",
    "\n",
    "```\n",
    "\n",
    "<b>Note:</b> \n",
    "1) The dataset file and save_model_path parameters are mandatory to be given, remaining parameters if not given will take the default values\n",
    "2) --help option will give the details of the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d1ddb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python src/run_modeltraining.py --batch_size 128 --dataset_file \"./data/ner_dataset.csv\" --intel 0 \\\n",
    "--save_model_path \"./models/trainedmodels/\"  | tee stock_training_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09377ad",
   "metadata": {},
   "source": [
    "To view the logged output, execute the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f225014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat stock_training_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c4c9f8",
   "metadata": {},
   "source": [
    "### Model Inference or Predictions\n",
    "The cell below will execute a python script to initiative inference in the kernel enabled above\n",
    "\n",
    "The script will run the benchmarks for the passed parameters and  displays the corresponding inference time in seconds. The details of the script and parameters are as follows:\n",
    "\n",
    "```shell\n",
    "python src/run_inference.py --batchsize <batchsize value> --dataset_file <dataset filename> -intel <intel/stock> --model_path <model file path>\n",
    "\n",
    "```\n",
    "\n",
    "  Arguments:<br>\n",
    "  \n",
    "```\n",
    "  --help                   show this help message and exit\n",
    "  --batchsize              Give the required batch sizes\n",
    "  --dataset_file           Give the name of test dataset file\n",
    "  --intel                  0 for stock, 1 for intel environment\n",
    "  --model_path             Give the directory path the trained model\n",
    "```\n",
    "\n",
    "Optionally, users can collect logs by redirecting to an output file as follows:\n",
    "\n",
    "```shell\n",
    "python src/run_inference.py --batchsize <batchsize value> --dataset_file <dataset filename> -intel <intel/stock> --model_path <model file path> | tee <log_file_name>\n",
    "```\n",
    "\n",
    "<b>Note:</b>\n",
    "1) All the options above are optional expect for test dataset file and model_path, if not given will take the default values\n",
    "2) --help option will give the details of the arguments<br>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e471ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/run_inference.py --batch_size 128 --dataset_file \"./data/ner_test_dataset.csv\" --intel 0 \\\n",
    "--model_path \"./models/trainedmodels/stock/model_b128/model_checkpoint\" | tee stock_inference_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3522a1db",
   "metadata": {},
   "source": [
    "To view the logged output, execute the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdbcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat stock_inference_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d073bc7e",
   "metadata": {},
   "source": [
    "## **Optimizing the E2E solution with Intel Optimizations for Tensorflow**\n",
    "### ***E2E Architecture***\n",
    "### **Use Case E2E flow**\n",
    "<img src=\"assets/e2e_flow_optimized.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cafc603",
   "metadata": {},
   "source": [
    "### ***Solution setup***\n",
    "Check that you are in the `ner_intel` kernel. If not, navigate to `Kernel > Change kernel > Python [conda env:ner_intel]` \\\n",
    "After activating the environment for stock Tensorflow framework, make sure the oneDNN flag is disabled by running the below cell to set the flag and verify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652cc470",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export TF_ENABLE_ONEDNN_OPTS=1\n",
    "echo $TF_ENABLE_ONEDNN_OPTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee48ec29",
   "metadata": {},
   "source": [
    "### ***Solution implementation***\n",
    "#### **Model building process with Intel® optimizations**\n",
    "The python script run_modeltraining.py used for training on stock version is used for training the model on Intel environment also as per example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/run_modeltraining.py --batch_size 128 --dataset_file \"./data/ner_dataset.csv\" --intel 1 \\\n",
    "--save_model_path \"./models/trainedmodels/\" | tee intel_training_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bed8b38",
   "metadata": {},
   "source": [
    "To view the logged output, execute the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deddecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat intel_training_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f19b8d",
   "metadata": {},
   "source": [
    "#### **Model Inference process with Intel® optimizations**\n",
    "The python script run_inference.py used to obtain inference benchmarks for stock version is used for getting inference benchmarks for Intel environment also as per example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc531a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/run_inference.py --batch_size 128 --dataset_file \"./data/ner_test_dataset.csv\" --intel 1 \\\n",
    "--model_path \"./models/trainedmodels/intel/model_b128/model_checkpoint\" | tee intel_inference_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8602c94",
   "metadata": {},
   "source": [
    "To view the logged output, execute the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acad795",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat intel_inference_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fece57",
   "metadata": {},
   "source": [
    "#### **Model Conversion process with Intel Neural Compressor**\n",
    "Intel® Neural Compressor is used to quantize the FP32 Model to the INT8 Model. \n",
    "Optimized model is used here for evaluating and timing Analysis. \n",
    "Intel® Neural Compressor supports many optimization methods. \n",
    "In this case, we have used post training quantization with default quantization method to quantize the FP32 model.\n",
    "\n",
    "Before performing the quantization of the trained model, the model is converted to frozen graph format using the <i>run_create_frozen_graph.py</i>\n",
    "python script. The usage of this script to generate the frozen graph is as follows:\n",
    "\n",
    "```\n",
    "python src/run_create_frozen_graph.py --model_path <trained model file path> --save_model_path <path to save the model frozen graph>\n",
    "```\n",
    "where,<br>\n",
    "<b>model_path</b> - The path of the FP32 trained model <br>\n",
    "<b>save_model_path</b> - The path to save the frozen graph format of the model given in model_path<br>\n",
    "\n",
    "The cell below provides an example execution this script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73a9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/run_create_frozen_graph.py --model_path \"./models/trainedmodels/intel/model_b128/model_checkpoint\" \\\n",
    "--save_model_path \"./models/frozen_models/intel/model_b32/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fcbcc5",
   "metadata": {},
   "source": [
    "Once the frozen model format of the model is created, the <i>run_neural_compressor_conversion.py</i> python script is used for \n",
    "quanitization of the FP32 trained model. The syntax for using the script\n",
    "given as follows:\n",
    "\n",
    "```\n",
    "python src/run_neural_compressor_conversion.py --dataset_file <test dataset file name> --model_path <path of the frozen graph> --config_file <configuration file> --save_model_path <path to save the model>\n",
    "```\n",
    "where,\n",
    "```\n",
    "--dataset_file        The path of the test dataset file\n",
    "--model_path          The path of the model file in the frozen graph format\n",
    "--config_file         The path of the configuration file which contains the settings for the quanitization\n",
    "--save_model_path     The path to save the quantized model\n",
    "```\n",
    "\n",
    "The cell below provides an example execution this script\n",
    "\n",
    ">Note:\n",
    "If while running the above script if the error \"Unable to run due to ImportError: libGL.so.1: \n",
    "cannot open shared object file: No such file or directory\" occurs then install libgl using the command,\n",
    "!sudo apt-get install libgl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4297d5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/INC/run_neural_compressor_conversion.py --dataset_file \"./data/ner_test_quan_dataset.csv\" \\\n",
    "--model_path \"./models/frozen_models/intel/model_b32/frozen_graph.pb\" --config_file \"./env/deploy.yaml\" \\\n",
    "--save_model_path \"./models/quantized_models/intel/model_b128_d100/inc_model_b128_d100/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273517be",
   "metadata": {},
   "source": [
    "If the quanitized model needs to be tuned to evaluate a specific accuracy relative to the FP32 trained model then the respective configuration parameters need to be set in the config file and the python script <i>run_neural_compressor_tune_conversion.py</i> is used in a similiar manner as <i>run_neural_compressor_conversion.py</i> except for the configuration file. The syntax of the script and usage is given as follows:\n",
    "\n",
    "```\n",
    "python src/INC/run_neural_compressor_tune_conversion.py --dataset_file <test dataset file name> --model_path <path of the frozen graph> --config_file <configuration file> --save_model_path <path to save the model>\n",
    "```\n",
    "where,\n",
    "```\n",
    "--dataset_file        The path of the test dataset file\n",
    "--model_path          The path of the model file in the frozen graph format\n",
    "--config_file         The path of the configuration file which contains the settings for the quanitization\n",
    "--save_model_path     The path to save the quantized model\n",
    "```\n",
    "The usage of the script is similar to the <i>run_neural_compressor_conversion.py</i> except for the configuration file.\n",
    "\n",
    "The cell below provides an example execution this script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/INC/run_neural_compressor_tune_conversion.py --dataset_file \"./data/ner_test_quan_dataset.csv\" \\\n",
    "--model_path \"./models/frozen_models/intel/model_b128/frozen_graph.pb\" --config_file \"./env/deploy_accuracy.yaml\" \\\n",
    "--save_model_path \"./models/acc_quantized_models/intel/model_b128_d100/inc_model_b128_d100/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc64855",
   "metadata": {},
   "source": [
    "#### **Model Inference process with Intel® Quanitizations**\n",
    "Now that the quantized model is created using INC it can be used for inferencing on the test data and perform benchmarking.\n",
    "The inferencing is done on the FP32 model and INC quantized model and results for real time inference and batch inference are used\n",
    "for benchmarking.\n",
    "\n",
    "The python script <i>run_neural_compressor_inference.py</i> is used for perform predictions on the test data. The syntax to use the script \n",
    "is given below.\n",
    "\n",
    "```\n",
    "python src/INC/run_neural_compressor_inference.py --batch_size 32 --dataset_file <test dataset file> --model_path <FP32 or INC frozen graph file>\n",
    "```\n",
    "where,\n",
    "```\n",
    "--batch_size      Give the required batch size for inference\n",
    "--dataset_file    The path of the test data set file name\n",
    "--model_path      The path of the FP32 or quantized frozen graph model\n",
    "```\n",
    "\n",
    "The cell below provides an example execution this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d145e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/INC/run_neural_compressor_inference.py --batch_size 128 --dataset_file \"./data/ner_test_quan_dataset.csv\" \\\n",
    "--model_path \"./models/quantized_models/intel/model_b128_d100/inc_model_b128_d100.pb\" | tee INC_inference_log.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c68a86",
   "metadata": {},
   "source": [
    "To view the logged output, execute the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523837af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat INC_inference_log.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ner_intel]",
   "language": "python",
   "name": "conda-env-ner_intel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
